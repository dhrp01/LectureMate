{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353efadc-f1ed-49c3-9214-41eed1eb825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80bc896-89b1-4d7e-b608-7a9f666be6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b714a071-0feb-422c-8831-a2d23d9dc140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753b5a0c5f1e4d59bb4fb2aa32fb8403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HF_access_token = \"hf_xxxx\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          token=HF_access_token, \n",
    "                                          padding_side=\"left\",\n",
    "                                          add_eos_token=True,\n",
    "                                          model_max_length=256)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=HF_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738377f6-aba5-4d69-bbd7-e5398172e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,207,552 || all params: 3,225,957,376 || trainable%: 0.4094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdalal_umass_edu/.local/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f8d209-af90-4f65-800e-4bad450bbd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"cs685\"  # Folder with text files containing lecture notes, papers, etc.\n",
    "texts = []\n",
    "\n",
    "# Load and preprocess text data\n",
    "for file in os.listdir(data_folder):\n",
    "    if os.path.isdir(file) or file == \".ipynb_checkpoints\":\n",
    "        continue\n",
    "    with open(os.path.join(data_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd2ac98-84e9-4fdf-b02c-ad116dc791d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  94317,   5127,  ...,   1095,    596,    636],\n",
       "        [128000,   1169,    596,  ...,    779,    584,    649],\n",
       "        [128000,  94317,   1095,  ...,   1690,   5627,   1314],\n",
       "        ...,\n",
       "        [128000,  94317,   1095,  ...,   2017,    279,  28223],\n",
       "        [128000,  94317,   1095,  ...,   2017,    279,  28223],\n",
       "        [128000,    198,   6014,  ...,   2763,    315,   4860]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the texts\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c63c93f-80b5-4751-93d1-683d247fe4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "labels = inputs[\"input_ids\"].clone()  # We use the same tokens as labels\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': inputs['input_ids'],\n",
    "    'attention_mask': inputs['attention_mask'],\n",
    "    'labels': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c839506-ef26-4e8f-a41e-b491b4ba91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 73/110 02:22 < 01:14, 0.50 it/s, Epoch 6.40/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.909300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6887c-f738-40f9-9a09-a80d16cb1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './fine_tuned_llama_model'\n",
    "model.save_pretrained(output_dir, safe_serialization=False)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe2bf05e-a9eb-4f5b-850e-7c83b260fc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): lora.Linear(\n",
       "    (base_layer): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    (lora_dropout): ModuleDict(\n",
       "      (default): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lora_A): ModuleDict(\n",
       "      (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "    )\n",
       "    (lora_B): ModuleDict(\n",
       "      (default): Linear(in_features=8, out_features=128256, bias=False)\n",
       "    )\n",
       "    (lora_embedding_A): ParameterDict()\n",
       "    (lora_embedding_B): ParameterDict()\n",
       "    (lora_magnitude_vector): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5addc3e3-6057-4648-bdbc-d2351704f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(prompt, max_tokens=350, temperature=0.7, repetition_penalty=1.2):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,      # Maximum number of tokens in the response\n",
    "            temperature=temperature,        # Controls randomness in response\n",
    "            repetition_penalty=repetition_penalty  # Discourages repeated phrases\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21d375e1-0523-4f42-8149-356b6cf4fd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/jdalal_umass_edu/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In the context of transfer learning, what does the process of fine-tuning specifically refer to?          a) Adjusting the learning rate during the pretraining phase           b) Modifying the architecture of the pretrained model to the better suit a specific task           c) training a pretrained model for a few additional epochs on a task-specific dataset           d) Replacing the transformer attention mechanism for specific tasks\\nThis question is about understanding  the concept of  transfer  learning and how  fine-tuning  a pre-trained model can be implemented. The correct option C states that in the process of  fine-tuning, the  pretrained  model is trained for a few more epochs on a task-specific dataset. This would update the model's parameters to make them more task-specific while keeping the rest of the model the same as in the pre-trained state. Option A is incorrect because it refers to something that happens during the  pre-training  phase. Option B is also incorrect since it refers to something that would involve significant reorganization of the model architecture, which is not what  fine-tuning  involves. Similarly, option D is incorrect because it refers to replacing a component of the model with a different implementation without changing the rest of the model.\\nThe best answer is C\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_model(\"In the context of transfer learning, what does the process of fine-tuning specifically refer to?\\\n",
    "          a) Adjusting the learning rate during the pretraining phase \\\n",
    "          b) Modifying the architecture of the pretrained model to the better suit a specific task \\\n",
    "          c) training a pretrained model for a few additional epochs on a task-specific dataset \\\n",
    "          d) Replacing the transformer attention mechanism for specific tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "70003ade-ab15-4d47-bcf9-ba780566169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Explain how the prompt tuning method discussed in class allows us to solve multiple different NLP tasks within a single batch\\nPrompt tuning  is  allowed  because  large  language  models  are  able  to  capture\\ncomplex  relationships  between  words  and  can  be  trained  to  predict  novel  tokens.\\nThis  ability  of  language  models  to  handle  outofvocabulary  tokens  enables  us  to\\ninstruct  the  model  to  predict  novel  tokens  that  are  specific  to  certain  nlp  tasks,\\nwhile  keeping  the  rest  of  the  architecture  and  training  procedure  same  as  before\\nbecause  the  cross-entropy  loss  function  used  during  training  no  longer  has  open-voyage\\ntokens. We also don’t have to update or retraining all the models that were mentioned earlier such\\nas sentiment analysis, question answering and text summarization since we are just updating the\\nlanguage model that was-trained on general purpose text without any nlp task specification.\\nalso note that all the instructions,attention masks, block size and other inputs are kept the\\nsame as before since we are just updating the pre-trained model on some additional instructions\\nthat are specific to the particular nlp task that we want to perform rather than updating the whole\\nmodel architecture which would require retraining all the models that were mentioned earlier uh and\\nwould take a lot long time if at all we could retrain all those models then maybe um if we wanted\\nto switch from say writing generations to editing generations for our uh language modeling\\ntask for instance we would have to retrain every single one of these models uh and that would\\nbe very difficult um and'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_model(\" Explain how the prompt tuning method discussed in class allows us to solve multiple different NLP tasks \\\n",
    "within a single batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d2d33b1-cebe-4c76-88c4-56319ce3191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'when were the midterm grades released? 11/8 update:  midterms are finally over for most students. 11 /8 update : uh so like i said before many of you have your own\\nInstructors for CS 685 Spring 2023\\nO n. 11 /8, we had 93 some students take a quiz to make up the first assignment um today we ﬁnalized all those quizzes and graded them uh as always we’re doing our best\\nto get these grades out as fast as possible while also allowing time for us to grade the homeworks that are due um on February whatever uh 16 or 17 so you’ll have an idea\\nof your homework grades pretty soon uh okay so with that let’s um see uh today we’re going to talk about oh God this is a very important\\ntopic um alignment ﬁrst from an instructor perspective um so we don’t have a template for our new ﬂip class um but uh we do have a\\nteam that is working on creating a template for the ﬂip version of our old class CS 15 in fact we’ve been using this template to build the\\nnew CS 685 um instance that we’re on right now so uh not too much chatty stuff let’s just start by talking about some of the\\nsecurity concerns uh that uh you have when um looking at pre-trained models for your language model and other models for your project um\\nso uh for anyone who wants to look at code I’ll give you a link at the end of this video um course website there is a version of the\\nproject example that uses Transformers 2.0 um which is what we recommend using for your projects as well it’s easier than the\\npre-trained model Hugging'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_model(\"when were the midterm grades released?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628e601-e260-4617-8459-4de7f146702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_model(\"What are encoder-decoder models?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
